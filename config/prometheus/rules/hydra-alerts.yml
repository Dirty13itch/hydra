groups:
  # =============================================================================
  # NODE HEALTH ALERTS
  # =============================================================================
  - name: node-health
    interval: 30s
    rules:
      - alert: NodeDown
        expr: up{job="node"} == 0
        for: 2m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Node {{ $labels.instance }} is down"
          description: "Node exporter on {{ $labels.instance }} has been unreachable for over 2 minutes."
          runbook_url: "https://github.com/hydra/docs/runbooks/node-down.md"

      - alert: NodeHighCPU
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | printf \"%.1f\" }}% on {{ $labels.instance }}."

      - alert: NodeCriticalCPU
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Critical CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | printf \"%.1f\" }}% on {{ $labels.instance }}. Immediate attention required."

      - alert: NodeHighMemory
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | printf \"%.1f\" }}% on {{ $labels.instance }}."

      - alert: NodeCriticalMemory
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
        for: 2m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Critical memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | printf \"%.1f\" }}% on {{ $labels.instance }}. OOM killer may activate."

      - alert: NodeHighLoad
        expr: node_load15 / count by(instance) (node_cpu_seconds_total{mode="idle"}) > 2
        for: 15m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High load average on {{ $labels.instance }}"
          description: "15-minute load average is {{ $value | printf \"%.2f\" }} per CPU on {{ $labels.instance }}."

  # =============================================================================
  # DISK ALERTS
  # =============================================================================
  - name: disk-health
    interval: 1m
    rules:
      - alert: DiskSpaceWarning
        expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes) * 100 < 20
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Low disk space on {{ $labels.instance }}:{{ $labels.mountpoint }}"
          description: "Disk space is {{ $value | printf \"%.1f\" }}% free on {{ $labels.mountpoint }}."

      - alert: DiskSpaceCritical
        expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes) * 100 < 10
        for: 2m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Critical disk space on {{ $labels.instance }}:{{ $labels.mountpoint }}"
          description: "Only {{ $value | printf \"%.1f\" }}% free on {{ $labels.mountpoint }}. Services may fail."

      - alert: DiskSpaceEmergency
        expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes) * 100 < 5
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "EMERGENCY: Disk nearly full on {{ $labels.instance }}:{{ $labels.mountpoint }}"
          description: "Only {{ $value | printf \"%.1f\" }}% free. Immediate action required!"

      - alert: DiskReadOnly
        expr: node_filesystem_readonly{fstype!~"tmpfs|overlay"} == 1
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Filesystem read-only on {{ $labels.instance }}:{{ $labels.mountpoint }}"
          description: "Filesystem has been remounted read-only, likely due to errors."

      - alert: DiskIOHigh
        expr: rate(node_disk_io_time_seconds_total[5m]) > 0.9
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High disk I/O on {{ $labels.instance }}"
          description: "Disk {{ $labels.device }} is saturated ({{ $value | printf \"%.1f\" }}% IO time)."

  # =============================================================================
  # GPU ALERTS
  # Supports both DCGM (hydra-ai) and nvidia-smi exporter (hydra-compute) formats
  # =============================================================================
  - name: gpu-health
    interval: 15s
    rules:
      - alert: GPUExporterDown
        expr: up{job="nvidia-gpu"} == 0
        for: 2m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "GPU exporter down on {{ $labels.instance }}"
          description: "Cannot collect GPU metrics from {{ $labels.instance }}."

      # Temperature alerts - nvidia-smi exporter format
      - alert: GPUTemperatureWarning
        expr: nvidia_gpu_temperature_celsius > 75
        for: 5m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "GPU temperature warning on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} is at {{ $value }}°C."

      # Temperature alerts - DCGM exporter format (hydra-ai)
      - alert: GPUTemperatureWarningDCGM
        expr: DCGM_FI_DEV_GPU_TEMP > 75
        for: 5m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "GPU temperature warning on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} is at {{ $value }}°C."

      - alert: GPUTemperatureCritical
        expr: nvidia_gpu_temperature_celsius > 83
        for: 2m
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "GPU overheating on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} is at {{ $value }}°C. Thermal throttling imminent!"

      - alert: GPUTemperatureCriticalDCGM
        expr: DCGM_FI_DEV_GPU_TEMP > 83
        for: 2m
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "GPU overheating on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} is at {{ $value }}°C. Thermal throttling imminent!"

      - alert: GPUTemperatureEmergency
        expr: nvidia_gpu_temperature_celsius > 90
        for: 30s
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "EMERGENCY: GPU critical temperature on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} at {{ $value }}°C. Risk of shutdown/damage!"

      - alert: GPUTemperatureEmergencyDCGM
        expr: DCGM_FI_DEV_GPU_TEMP > 90
        for: 30s
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "EMERGENCY: GPU critical temperature on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} at {{ $value }}°C. Risk of shutdown/damage!"

      # VRAM alerts - nvidia-smi exporter format (90% warning)
      - alert: GPUMemoryWarning
        expr: (nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes) * 100 > 90
        for: 5m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "GPU VRAM usage high on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} memory is {{ $value | printf \"%.1f\" }}% used. Consider unloading models."

      # VRAM alerts - DCGM format (90% warning) - uses MiB
      - alert: GPUMemoryWarningDCGM
        expr: (DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL) * 100 > 90
        for: 5m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "GPU VRAM usage high on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} memory is {{ $value | printf \"%.1f\" }}% used. Consider unloading models."

      # VRAM alerts - nvidia-smi exporter format (95% critical)
      - alert: GPUMemoryCritical
        expr: (nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes) * 100 > 95
        for: 2m
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "GPU VRAM nearly exhausted on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} memory is {{ $value | printf \"%.1f\" }}% used. OOM risk imminent!"

      # VRAM alerts - DCGM format (95% critical)
      - alert: GPUMemoryCriticalDCGM
        expr: (DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL) * 100 > 95
        for: 2m
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "GPU VRAM nearly exhausted on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} memory is {{ $value | printf \"%.1f\" }}% used. OOM risk imminent!"

      # VRAM alerts - Emergency (98%) for tight margins on large GPUs (RTX 5090 32GB)
      - alert: GPUMemoryEmergency
        expr: (nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes) * 100 > 98
        for: 30s
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "EMERGENCY: GPU VRAM at capacity on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} at {{ $value | printf \"%.1f\" }}% VRAM. OOM imminent! Immediate action required."
          runbook_url: "https://github.com/hydra/docs/runbooks/gpu-oom.md"

      # VRAM alerts - DCGM Emergency (98%)
      - alert: GPUMemoryEmergencyDCGM
        expr: (DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL) * 100 > 98
        for: 30s
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "EMERGENCY: GPU VRAM at capacity on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} at {{ $value | printf \"%.1f\" }}% VRAM. OOM imminent! Immediate action required."
          runbook_url: "https://github.com/hydra/docs/runbooks/gpu-oom.md"

      # Power alerts - nvidia-smi format (500W for RTX 5090/4090 compatibility)
      - alert: GPUPowerHigh
        expr: nvidia_gpu_power_draw_watts > 500
        for: 10m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "High GPU power draw on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} drawing {{ $value | printf \"%.0f\" }}W."

      # Power alerts - DCGM format (500W threshold)
      - alert: GPUPowerHighDCGM
        expr: DCGM_FI_DEV_POWER_USAGE > 500
        for: 10m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "High GPU power draw on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} drawing {{ $value | printf \"%.0f\" }}W."

      # Emergency power alert - sustained high draw (UPS protection)
      - alert: GPUPowerCritical
        expr: nvidia_gpu_power_draw_watts > 550 or DCGM_FI_DEV_POWER_USAGE > 550
        for: 5m
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "Critical GPU power draw on {{ $labels.instance }}"
          description: "GPU {{ $labels.gpu }} drawing {{ $value | printf \"%.0f\" }}W. Check UPS capacity (2000W total)."

      - alert: GPUNotDetected
        expr: count(nvidia_gpu_temperature_celsius) by (instance) == 0 and count(DCGM_FI_DEV_GPU_TEMP) by (instance) == 0
        for: 5m
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "No GPUs detected on {{ $labels.instance }}"
          description: "nvidia-smi/DCGM reports no GPUs. Driver issue or hardware failure."

  # =============================================================================
  # INFERENCE SERVICE ALERTS
  # =============================================================================
  - name: inference-health
    interval: 30s
    rules:
      - alert: TabbyAPIDown
        expr: up{job="tabbyapi"} == 0
        for: 2m
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "TabbyAPI is down"
          description: "Primary inference engine on hydra-ai has been unreachable for over 2 minutes."

      - alert: TabbyAPINoModel
        expr: tabby_model_loaded == 0
        for: 5m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "TabbyAPI has no model loaded"
          description: "TabbyAPI is running but no model is currently loaded."

      - alert: TabbyAPISlowInference
        expr: histogram_quantile(0.95, rate(tabby_inference_duration_seconds_bucket[5m])) > 10
        for: 10m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "Slow inference on TabbyAPI"
          description: "95th percentile inference latency is {{ $value | printf \"%.1f\" }}s."

      - alert: OllamaDown
        expr: probe_success{job="blackbox", target=~".*11434.*"} == 0
        for: 2m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "Ollama is down"
          description: "Ollama service on hydra-compute is unreachable."

      - alert: LiteLLMDown
        expr: up{job="litellm"} == 0 or probe_success{job="blackbox", target=~".*4000.*"} == 0
        for: 2m
        labels:
          severity: critical
          team: ai
        annotations:
          summary: "LiteLLM proxy is down"
          description: "API gateway is unreachable. All inference routing is affected."

  # =============================================================================
  # DATABASE ALERTS
  # =============================================================================
  - name: database-health
    interval: 30s
    rules:
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 2m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database server is unreachable."

      - alert: PostgreSQLHighConnections
        expr: sum(pg_stat_activity_count) by (instance) / pg_settings_max_connections * 100 > 80
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "PostgreSQL connection pool nearly exhausted"
          description: "{{ $value | printf \"%.0f\" }}% of max connections in use."

      - alert: PostgreSQLSlowQueries
        expr: rate(pg_stat_activity_max_tx_duration{state="active"}[5m]) > 60
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "PostgreSQL has slow queries"
          description: "Long-running queries detected on PostgreSQL."

      - alert: RedisDown
        expr: redis_up == 0
        for: 2m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Redis is down"
          description: "Redis cache server is unreachable."

      - alert: RedisHighMemory
        expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 90
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Redis memory usage high"
          description: "Redis using {{ $value | printf \"%.0f\" }}% of max memory."

      - alert: QdrantDown
        expr: probe_success{job="blackbox", target=~".*6333.*"} == 0
        for: 2m
        labels:
          severity: warning
          team: ai
        annotations:
          summary: "Qdrant vector database is down"
          description: "Qdrant is unreachable. RAG functionality affected."

  # =============================================================================
  # DOCKER/CONTAINER ALERTS
  # =============================================================================
  - name: container-health
    interval: 30s
    rules:
      - alert: ContainerDown
        expr: container_last_seen{name!=""} < (time() - 60)
        for: 2m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Container {{ $labels.name }} is down"
          description: "Container has not been seen for over 2 minutes."

      - alert: ContainerHighCPU
        expr: sum(rate(container_cpu_usage_seconds_total{name!=""}[5m])) by (name) * 100 > 80
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Container {{ $labels.name }} using high CPU"
          description: "Container using {{ $value | printf \"%.1f\" }}% CPU."

      - alert: ContainerHighMemory
        expr: container_memory_usage_bytes{name!=""} / container_spec_memory_limit_bytes * 100 > 90
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Container {{ $labels.name }} memory high"
          description: "Container using {{ $value | printf \"%.0f\" }}% of memory limit."

      - alert: ContainerRestartLoop
        expr: increase(container_restart_count{name!=""}[1h]) > 5
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Container {{ $labels.name }} restarting frequently"
          description: "Container has restarted {{ $value }} times in the last hour."

      - alert: ContainerOOMKilled
        expr: container_oom_events_total > 0
        for: 1m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Container {{ $labels.name }} OOM killed"
          description: "Container was killed due to out of memory."

  # =============================================================================
  # NETWORK ALERTS
  # =============================================================================
  - name: network-health
    interval: 30s
    rules:
      - alert: NFSMountStale
        expr: node_filesystem_readonly{fstype="nfs4"} == 1
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "NFS mount read-only on {{ $labels.instance }}"
          description: "NFS mount {{ $labels.mountpoint }} is stale or read-only."

      - alert: NFSMountMissing
        expr: absent(node_filesystem_size_bytes{mountpoint="/mnt/models"})
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "NFS models mount missing"
          description: "/mnt/models is not mounted. Inference will fail."

      - alert: HighNetworkTraffic
        expr: rate(node_network_receive_bytes_total{device!~"lo|docker.*|veth.*|tailscale.*|br-.*|tunl.*"}[5m]) > 1e9
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High network traffic on {{ $labels.instance }}"
          description: "Receiving {{ $value | humanize }}B/s on {{ $labels.device }}."

      - alert: NetworkInterfaceDown
        # Exclude: loopback, docker networks, veth, tailscale, bridges, tunnels, wifi, unused NICs
        expr: node_network_up{device!~"lo|docker.*|veth.*|tailscale.*|br-.*|tunl.*|wlp.*|enp12s0"} == 0
        for: 2m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Network interface down on {{ $labels.instance }}"
          description: "Interface {{ $labels.device }} is down."

  # =============================================================================
  # STORAGE SYSTEM ALERTS (Unraid specific)
  # =============================================================================
  - name: storage-health
    interval: 1m
    rules:
      - alert: UnraidArrayDegraded
        expr: unraid_array_health != 1
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Unraid array is degraded"
          description: "Storage array is not in healthy state. Check disk status."

      - alert: UnraidParityCheckRunning
        expr: unraid_parity_check_running == 1
        for: 1m
        labels:
          severity: info
          team: infrastructure
        annotations:
          summary: "Unraid parity check in progress"
          description: "Performance may be degraded during parity check."

      - alert: UnraidDiskError
        expr: unraid_disk_errors > 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Unraid disk has errors"
          description: "Disk {{ $labels.disk }} has {{ $value }} errors."

  # =============================================================================
  # PROMETHEUS SELF-MONITORING
  # =============================================================================
  - name: prometheus-health
    interval: 30s
    rules:
      - alert: PrometheusTargetDown
        expr: up == 0
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Prometheus target {{ $labels.job }} down"
          description: "Cannot scrape {{ $labels.instance }} for job {{ $labels.job }}."

      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Prometheus config reload failed"
          description: "Prometheus failed to reload its configuration."

      - alert: PrometheusTSDBCompactionsFailed
        expr: increase(prometheus_tsdb_compactions_failed_total[1h]) > 0
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Prometheus TSDB compaction failures"
          description: "Prometheus has had {{ $value }} compaction failures."

      - alert: PrometheusHighCardinality
        expr: prometheus_tsdb_head_series > 1000000
        for: 15m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Prometheus high cardinality"
          description: "Prometheus has {{ $value }} active series."
